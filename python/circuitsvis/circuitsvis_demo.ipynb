{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"mdCsqhXBg-IF"},"source":["# CircuitsVis: extensions\n","\n","This notebook contains my fork of circuitsvis. It includes a few extra features relative to the standard version, such as:\n","\n","* It can be run on a TransformerLens `ActivationCache` rather than just on a tensor of attention patterns (all examples in the notebook do this),\n","* Option to show value-weighted attention patterns rather than regular patterns,\n","* It can toggle between multiple sequences in a batch,\n","* Plots can be opened in browser by default rather than displayed inline (won't work in Colab),\n","* Added [bertviz](https://github.com/jessevig/bertviz)-style plots, with some extra features.\n","\n","Please comment or send me a message if you have any feedback. I hope you find this useful!\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/CallumMcDougall_circuit_visualisations_colorful_clear_simple_bo_6c1b2148-84e2-4ab8-906e-d104cdc85e06.png\" width=\"320\">"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_cQSNp3KZ5w6"},"source":["# Setup code (don't read, just run)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Mhat43AVZ5w9"},"outputs":[],"source":["try:\n","    import google.colab\n","    # %pip install jaxtyping\n","    %pip install transformer_lens\n","    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n","except:\n","    import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n","    from IPython import get_ipython\n","    ipython = get_ipython()\n","    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","    ipython.run_line_magic(\"autoreload\", \"2\")\n","\n","import torch as t\n","from torch import Tensor\n","import circuitsvis as cv\n","from jaxtyping import Float\n","from IPython.display import clear_output, display\n","from transformer_lens import HookedTransformer\n","\n","t.set_grad_enabled(False)\n","\n","gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\")\n","clear_output()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"51uwwxCdZ5w-"},"outputs":[],"source":["sentence0 = \"When Mary and John went to the shops, John gave a drink to Mary.\"\n","sentence1 = \"When Mary and John went to the shops, Mary gave a drink to John.\"\n","sentence2 = \"The cat sat on the mat.\"\n","sentences_all = [sentence0, sentence1, sentence2]\n","\n","names_filter = lambda name: any(name.endswith(f\"hook_{s}\") for s in [\"pattern\", \"q\", \"k\", \"v\"])\n","logits, cache = gpt2.run_with_cache(sentence0, names_filter=names_filter, remove_batch_dim=True)\n","logits_all, cache_all = gpt2.run_with_cache(sentences_all, names_filter=names_filter)\n","logits, cache_full = gpt2.run_with_cache(sentence0, remove_batch_dim=True)\n","\n","attn: Float[Tensor, \"layers heads seq_Q seq_K\"] = t.stack([cache[\"pattern\", i] for i in range(gpt2.cfg.n_layers)])\n","attn_all: Float[Tensor, \"batch layers heads seq_Q seq_K\"] = t.stack([cache_all[\"pattern\", i] for i in range(gpt2.cfg.n_layers)], dim=1)\n","\n","tokens = gpt2.to_str_tokens(sentence0)\n","tokens_all = gpt2.to_str_tokens(sentences_all)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wR1Wx-DmZ5w-"},"source":["# Overview of function\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wIu64WFJbU48"},"source":["The main function is `cv.attention.from_cache`. The most important arguments are:\n","\n","* `cache` - the `ActivationCache` object. This has to contain the appropriate activations (i.e. `pattern`, plus `v` if you're using value-weighted attention, plus `q` and `k` if you're using `lines` mode).\n","* `tokens` - either a list of strings (if batch size is 1), or a list of lists of strings (if batch size is > 1).\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yUzcS2VRbWHz"},"source":["The optional arguments are:\n","\n","* `heads` - if specified (e.g. `[(9, 6), (9, 9)]`), these heads will be shown in the visualisation. If not specified, behaviour is determined by the `layers` argument.\n","* `layers` - this can be an int (= single layer), list of ints (= list of layers), or None (= all layers). If `heads` are not specified, then the value of this argument determines what heads are shown.\n","* `batch_idx` - if the cache has a batch dimension, then you can specify this argument (as either an int, or list of ints). Note that you can have nontrivial batch size in your visualisations (you'll be able to select different sequences using a dropdown).\n","* `use_value_weighted_attn` - if True, then the visualisation will use value-weighted attention, i.e. every attention probability $A^h[s_Q, s_K]$ will be replaced with:\n","\n","$$\n","A^h[s_Q, s_K] \\times \\frac{\\|v^h[s_K]\\|}{\\underset{s}{\\max} \\|v^h[s]\\|}\n","$$\n","\n","* `mode` - this can be \"large\", \"small\" or \"lines\", for producing the three different types of attention plots (see below for examples of all).\n","* `return_mode` - this can be \"browser\" (open plot in browser; doesn't work in Colab or VMs), \"html\" (returns html object), or \"view\" (displays object inline).\n","* `radioitems` - if True, you select the sequence in the batch using radioitems rather than a dropdown. Defaults to False.\n","* `title` - if given, then a title is appended to the start of your plot (i.e. an `<h1>` HTML item).\n","* `help` - if True, prints out a string explaining the visualisation. Defulats to False."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gNmjzhjEZ5w_"},"source":["# Examples\n","\n","Below is a set of examples, along with some brief explanations."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L4qafzzLjvYW"},"source":["## Default arguments (batch vs no batch)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VO54Lj-TZ5w_"},"source":["\n","The first 2 examples below show how the function works when you don't specify any other keyword arguments (for cache having a batch dim vs not having a batch dim).\n","\n","The third shows how you can also specify a single element in a batch, using the `batch_idx` argument."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"executionInfo":{"elapsed":1144,"status":"ok","timestamp":1688585737827,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"93SNPRF8Z5w_","outputId":"28dfe18f-6e39-4a5d-c70a-4804a4c11b3c"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":791},"executionInfo":{"elapsed":1870,"status":"ok","timestamp":1688585739695,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"ewk44u5LZ5w_","outputId":"d19e1326-180d-4375-c96a-bfe29e880a59"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1688585739695,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"RVx4dL1xZ5xA","outputId":"1be20c5f-f48e-474b-a03b-115e0c118ab4"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    batch_idx = 1, # Different way to speciful a sequence within batch\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OWAlW_14Z5xA"},"source":["## Different ways to specify heads\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Q0sRqbLKjtRG"},"source":["We can specify heads using the `layer` argument, or the `heads` argument."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1688585739696,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"xND4kBmuZ5xA","outputId":"534b08c7-bc4e-483d-8c2f-3a9eefa376ab"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    layers = [9, 10],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1688585739696,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"QntJRlEyZ5xB","outputId":"4976bc3f-f194-437f-bdea-d79add3b577e"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    heads = [(9, 6), (9, 9), (10, 0)], # Showing all the name mover heads: `to` attends to the IO token `Mary`\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4g7LcILIjpHG"},"source":["## Different modes: `\"large\"` and `\"lines\"`\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fKKk3lAaZ5xB"},"source":["\n","The mode above is \"small\" (also known as `attention_patterns` in circuitsvis). You can also use \"large\" (which is `attention_heads` in circuitsvis), or \"lines\" (which is like the neuron view in [bertviz](https://github.com/jessevig/bertviz), but with a few extra features I added)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":864},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1688585739696,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"LbBZEzyrZ5xB","outputId":"a2991ded-7bee-4c76-ee3d-5ebd2299d683"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    mode = \"large\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"output_embedded_package_id":"169DOZJxlteO0SgFGwAbTqi8DfM5Gq3mX"},"executionInfo":{"elapsed":7768,"status":"ok","timestamp":1688585747455,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"HQC1NbOJZ5xB","outputId":"a2d2f560-306b-46e5-e276-629e488fb9e2"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    mode = \"lines\",\n","    display_mode = \"dark\", # Can also choose \"light\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"output_embedded_package_id":"1LLFhRGNx_4OObMsRNr-Q7UjDcDbNBsiC"},"executionInfo":{"elapsed":8725,"status":"ok","timestamp":1688585756177,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"B5jFwh5JZ5xB","outputId":"46a8974c-18b1-47ac-cfd2-33c1a2507656"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    mode = \"lines\",\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2OZzBEuxjnBz"},"source":["## Value-weighted attention\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j8uFtMYhZ5xB"},"source":["Value-weighted attention is a pretty neat concept. TL;DR - **if a value vector doubled in magnitude but attention probability halved then nothing would change; so we should expect the attention probability to be more meaningful once we scale by the magnitude of the value vector**. This is what the `use_value_weighted_attn` argument does; every attention probability $A^h[s_Q, s_K]$ will be replaced with:\n","\n","$$\n","A^h[s_Q, s_K] \\times \\frac{\\|v^h[s_K]\\|}{\\underset{s}{\\max} \\|v^h[s]\\|}\n","$$\n","\n","where $\\|v^h[s]\\|$ is the $L_2$ norm of the value vector at source position $s$ in head $h$.\n","\n","In particular, we see that the attention on the BOS token is much lower (because usually this token is attended to as a placeholder, and not much is actually copied). The example below shows GPT2-small's 3 main **name mover heads**; they attend from the `to` token (as well as some other places) to the first `Mary` token, and copy it as a prediction. We can see from these patterns that this is basically all that heads `9.6` and `9.9` do (`10.7` is a little messier but still roughly the same result holds)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1688585756177,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"UDGPt4QXZ5xB","outputId":"17eb4081-263f-4964-9394-ba10ab340a02"},"outputs":[],"source":["cv.attention.from_cache(\n","    cache = cache,\n","    tokens = tokens,\n","    heads = [(9, 6), (9, 9), (10, 0)], # showing all the name mover heads: `to` attends to the IO token `Mary`\n","    attention_type = \"info-weighted\", # or try \"value-weighted\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TBGtYCkgjlC5"},"source":["## Other arguments"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UrpKtSOwZ5xC"},"source":["`title` (optional) specifies a title.\n","\n","If `help` is True, then a string explaining the visualisation is printed out (as well as an explanation of the non-default arguments which you're using).\n","\n","If `radioitems` is True, then you select different sequences in the batch using radioitems rather than a dropdown. Defaults to False.\n","\n","`display_mode` can be \"dark\" (default) or \"light\". This only affects the \"lines\" mode.\n","\n","`return_mode` can be \"browser\" (open in browser), \"html\" (return html object), or \"view\" (display object inline; this is default).\n","\n","Note that the browser view is often preferable - it doesn't slow down your IDE, and it can reduce flickering when you switch between different sequences in your batch. However, this won't always work (e.g. in virtual machines or on Colab). In this case, you should use `return_mode = \"html\"`, then save the result and download & open it manually. Example code is given below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":758,"output_embedded_package_id":"17d-e_wInpsJUQelUQS2uzEXC5q-Hii6D"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1688585756178,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"HYuN8VpMZ5xC","outputId":"40734aca-a9be-4083-9239-72b1af679121"},"outputs":[],"source":["html_object = cv.attention.from_cache(\n","    cache = cache_all,\n","    tokens = tokens_all,\n","    batch_idx = 0,\n","    heads = [(9, 6), (9, 9), (10, 0)],\n","    mode = \"lines\",\n","    title = \"Attention of name mover heads (lines mode)\",\n","    return_mode = \"html\",\n","    help = True,\n","    radioitems = True,\n","    display_mode = \"light\", # This might be better if you're opening in browser\n",")\n","\n","display(html_object)\n","\n","with open(\"my_file.html\", \"w\") as f:\n","    f.write(html_object.data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Attribution plots (work-in-progress!)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Attention plots might also be decent for logit attribution. The values of each cell are \"logits directly written in the correct direction\".\n","\n","You can pass the `resid_directions` argument to the function, and it'll measure the attribution in that direction (e.g. this could be a `logit_diff` vector). Note that it also supports `resid_directions` being a vector of length `d_vocab`; then it can convert this into a vector in the residual stream by mapping it backwards through `W_U` (and this also means we can do logit attribution for the unembedding bias `b_U`).\n","\n","There are 2 really hacky things about this function, which are why I don't recommend people use it yet.\n","\n","1. The colors are awkward; lots of translations and scalings have to be done to make sure that (1) the whitepoint is zero logit attribution and (2) no values are more than 1. The way I did this is by dividing all the component logit attributions by the maximum absolute value of all components' contributions over all sequence positions (which leads to a sparser & cleaner plot, and also makes it easier to make relative comparisons between different values). For the large plot `attention_heads`, this was sufficient, because it also supports negative values: `[-1, 0]` is red and `[0, 1]` is blue. For the smaller plot `attention_patterns`, there's only one color shade for each facet plot, and so this one has to be split up into 2 separate plots.\n","\n","2. If you're just doing attribution at a particular sequence position, it makes sense to just have a `(1, seq_len)`-shape plot per component, rather than a `(seq_len, seq_len)`-size plot where you only care about one row. Currently, circuitsvis doesn't support having different source and destination tokens so this isn't yet possible. A hacky solution: whenever `resid_directions` is just a single vector (i.e. it doesn't have a `seq_len` dimension), I broadcast it along the square attention plots, so you see a vertical stripe rather than just a dot. Uncomment the lines below to see this in action.\n","\n","I expect both of these two things to be solved eventually, but they're not high on my priority list right now."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["resid_directions = t.zeros(gpt2.cfg.d_vocab, dtype=t.float32, device=gpt2.cfg.device)\n","resid_directions[gpt2.to_single_token(\" John\")] = -1\n","resid_directions[gpt2.to_single_token(\" Mary\")] = +1\n","\n","end_token_position = len(tokens) - 1 - tokens[::-1].index(\" to\")\n","\n","html_pos, html_neg = cv.attribution.from_cache(\n","    model = gpt2,\n","    cache = cache_full,\n","    tokens = tokens,\n","    mode = \"small\",\n","    # seq_pos = end_token_position,\n","    # resid_directions = resid_directions,\n",")\n","display(html_pos)\n","display(html_neg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["html_all = cv.attribution.from_cache(\n","    model = gpt2,\n","    cache = cache_full,\n","    tokens = tokens,\n","    mode = \"large\",\n","    # seq_pos = end_token_position,\n","    # resid_directions = resid_directions,\n",")\n","display(html_all)"]}],"metadata":{"colab":{"collapsed_sections":["_cQSNp3KZ5w6","wR1Wx-DmZ5w-","gNmjzhjEZ5w_","L4qafzzLjvYW","OWAlW_14Z5xA","4g7LcILIjpHG","2OZzBEuxjnBz","TBGtYCkgjlC5"],"provenance":[]},"kernelspec":{"display_name":"tl_intro_test","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
